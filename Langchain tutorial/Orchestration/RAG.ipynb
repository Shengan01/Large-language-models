{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "if not os.environ.get(\"LANGSMITH_API_KEY\"):\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter API key for LangSmith\")\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider='openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk contesnts of webpage\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths = ('https://lilianweng.github.io/posts/2023-06-23-agent/',),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=('post-content', 'post-title', 'post-header')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Defining prompt for question-answering\n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state['question'])\n",
    "    return {'context': retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = '\\n\\n'.join(doc.page_content for doc in state['context'])\n",
    "    messages = prompt.invoke({'question': state['question'], 'context': docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {'answer': response.content}\n",
    "\n",
    "\n",
    "# Complile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, 'retrieve')\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complex task into smaller, manageable sub-tasks or steps. It can be achieved through techniques like simple prompting, task-specific instructions, or human inputs, and often involves methods such as Chain of Thought and Tree of Thoughts to enhance reasoning. This approach helps in organizing and simplifying the execution of complicated tasks.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({'question': \"What is Task Decomposition?\"})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "bs4_strainer = bs4.SoupStrainer(class_=('post-title', 'post-header', 'post-content'))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=('https://lilianweng.github.io/posts/2023-06-23-agent/',),\n",
    "    bs_kwargs={'parse_only': bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f'Total characters: {len(docs[0].page_content)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    add_start_index = True,\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f'Split blog post into {len(all_splits)} sub-documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['347ff8a4-2e78-4e92-a44a-9c430e213631', '2ec4b770-65af-44cf-99ba-2230fdce5f25', 'fbdc641c-4573-4428-bc04-ead0c2c7d996']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents = all_splits)\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {'context': '(context goes here)', 'question': '(question goes here)'}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
